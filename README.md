# ChatwithDoc - RAG Pipeline for Document Querying

ChatwithDoc is a powerful Retrieval-Augmented Generation (RAG) pipeline that allows users to upload documents in various formats (.docx, .pdf, .txt) and interact with their contents using a Large Language Model (LLM). The system processes and retrieves relevant information efficiently, making document querying seamless and user-friendly.

---

## Features

- **Multi-format Document Support**: Accepts `.docx`, `.pdf`, and `.txt` files.
- **Efficient Embedding and Storage**: Leverages an embedding model to tokenize documents and stores their vector representations in ChromaDB, a robust vector database.
- **Smart Retrieval**: Implements a K-nearest neighbors algorithm (k=3) for efficient chunk retrieval based on user queries.
- **LLM-Powered Responses**: Provides precise answers by passing only the most relevant chunks to the LLM.

---

## Technologies Used

- **[LangChain](https://langchain.com/)**: Framework for building applications powered by language models.
- **[ChromaDB](https://www.trychroma.com/)**: Vector database for embedding storage and retrieval.
- **Python**: Core language for developing the pipeline.

---

## Workflow

1. **Document Upload**:
   The user uploads a document in `.docx`, `.pdf`, or `.txt` format.

2. **Embedding Generation**:
   The document is processed using an embedding model, converting its content into tokens and vector representations.

3. **Vector Storage**:
   The generated vectors are stored in ChromaDB for efficient similarity-based retrieval.

4. **Query Input**:
   The user submits a query related to the document's content.

5. **Chunk Retrieval**:
   The system retrieves the top 3 most relevant chunks using a K-nearest neighbors algorithm.

6. **Response Generation**:
   The retrieved chunks are passed to the LLM, which generates a precise response to the user's query.

---

## How It Works

1. **Input a Document**: Upload any document in the supported formats.
2. **Ask a Question**: Pose a query related to the document.
3. **Receive a Response**: Get an accurate, context-aware answer generated by the LLM.

---

## Getting Started

1. Clone the repository:
   ```bash
   git clone https://github.com/3xpl0itk1t/ChatwithDoc.git
## Installation
1. **Setup new Python env or use my existing one**:
    ```bash
    python -m venv <your_env_name>

            OR

   source ./myenv/bin/activate
2. **Install the required dependencies**:
   ```bash
   pip install -r requirements.txt
## Run the Application

1. Run the application:
   ```bash
   python app.py
## Upload a document and start chatting with your data!

## Future Enhancements

- Support for additional document formats (e.g., `.xlsx`, `.csv`).
- Improved chunking and retrieval mechanisms.
- Enhanced user interface for seamless interaction.

## Contributing

We welcome contributions to improve ChatwithDoc! Please open an issue or submit a pull request.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Contact

For questions or feedback, feel free to reach out to us at prabhavmishra7@gmail.com
